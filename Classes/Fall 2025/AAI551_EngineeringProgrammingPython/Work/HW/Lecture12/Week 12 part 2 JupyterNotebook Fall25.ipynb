{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 11 lecture focused on:\n",
    "1) ``__iter__``, ``__next__``, ``__len__``, ``__max__``, ``__eq__``, ``__lt__``, ``__gt__``\n",
    "2) Three ways to combine classes:\n",
    "   \n",
    "   -- Inheritance: “Is-A” relationships\n",
    "   \n",
    "   -- Composition: “Has-A” relationships\n",
    "   \n",
    "   -- Delegation: “Like-A” relationships\n",
    "3) Revisit ``getattr()`` and ``__getattrt__()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarification about **composition** and **delegation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture focuses on:\n",
    "1) Data Structures: Array, Linked List, Stack, Queue\n",
    "2) Library: PyTorch: Tensors, Autograd, optimizer, neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array in Python "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) list (array-like, not exactly)\n",
    "2) array.array (from the array module)\n",
    "3) numpy.array (from NumPy; best for scientific computing)\n",
    "4) torch.tensor (from PyTorch; similar to NumPy arrays but with GPU acceleration and autograd support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [1, 2, 3, \"List allows mixed data types.\", (4, 5)]\n",
    "print(arr1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [1, 2, 3]\n",
    "(id(arr1))\n",
    "print(id(arr1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "\"\"\" \n",
    "array.array is not dynamically typed — it’s type-constrained.\n",
    "All elements in the array must be of the same type, specified by the type code.\n",
    "'i'\tSigned integer (2 or 4 bytes)\n",
    "'f'\tFloat (4 bytes)\n",
    "'d'\tDouble float (8 bytes)\n",
    "\"\"\"\n",
    "\n",
    "arr2 = array.array('i', [1, 2, 3, 4])\n",
    "print(arr2[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array.array is more memory-efficient. It stores the raw C value directly, e.g., 4-byte int, in a single, contiguous block of memory. There is no separate object for each element.\n",
    "\n",
    "#### A list is just a container. It stores every element as a full object with significant overhead (references, types, and values). Thus, objects/elements may be scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import sys\n",
    "\n",
    "a = list(range(10000))             # Elements are often NOT stored contiguously\n",
    "b = array.array('i', range(10000)) # Elements are always stored contiguously\n",
    "print(sys.getsizeof(a))            # The size of an object in bytes\n",
    "print(sys.getsizeof(b))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr3 = np.array([1, 2, 3, 4])      # Elements are always stored contiguously\n",
    "print(arr3[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-D Array (Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "    ]\n",
    "print(arr[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "row1 = array.array('i', [1, 2, 3, 4])\n",
    "row2 = array.array('i', [5, 6, 7, 8])\n",
    "row3 = array.array('i', [9, 10, 12, 12])\n",
    "arr = [row1, row2, row3]\n",
    "print(arr[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "print(arr[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.shape)   # Describes its dimensions and the size of each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4])     # Creates a 1-D NumPy array from a flat Python list\n",
    "print((arr.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1]])            # Creates a 2-D NumPy array, 1 X 1\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 2, 3, 4]])   # Creates a 2-D NumPy array, 1 X 4\n",
    "print(arr.shape)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.sum(axis = 0))  # 0: The first dimension (rows)\n",
    "                          # [sum of col 0, col 1, col 2, col 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.sum(axis = 1))  # 1: The second dimension (columns)\n",
    "                          # [sum of row 0, row 1, row 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 2, 3, 4]])\n",
    "arr1 = np.array([[2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr + arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linked Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linked lists are made up pf **nodes**, where each node object stores **data** and a **link to the next node**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a node class that automatically initializes its own data and next. The defaults values are None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, next = None):\n",
    "        self.data = data\n",
    "        self.next = next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instances node1, node2, and node3 from class Node with data 1, 2, and 3, repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = Node(1)\n",
    "node2 = Node(2)\n",
    "node3 = Node(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(node2))\n",
    "print(id(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now link the nodes such that node1 -> node2 -> node3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1.next = node2\n",
    "node2.next = node3\n",
    "node3.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"({node1.data},{id(node1.next)})\")\n",
    "print(f\"({node2.data},{id(node2.next)})\")\n",
    "print(f\"({node3.data},{node3.next})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference of the third node is None, which indicates that it is the end of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a method ``printList()`` that takes the head of the list as an argument and prints each node until it gets to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, next = None):\n",
    "        self.data = data\n",
    "        self.next = next\n",
    "\n",
    "    def __str__(self):\n",
    "        nxt = id(self.next) if self.next else None\n",
    "        return f\"({self.data}, {nxt})\"\n",
    "\n",
    "    def printList(self):\n",
    "        current = self\n",
    "        while current:\n",
    "            print(current)\n",
    "            current = current.next   # At the 3rd iteration, current = None, exit the loop\n",
    "\n",
    "node1 = Node(1)\n",
    "node2 = Node(2)\n",
    "node3 = Node(3)\n",
    "\n",
    "node1.next = node2\n",
    "node2.next = node3\n",
    "node3.next = None\n",
    "\n",
    "node1.printList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class ``LinkedList`` whose attributes are ``head`` and ``length``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedList():\n",
    "    def __init__(self):\n",
    "        self.head = None          \n",
    "        self.length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ``appendNode()``, ``__len__()``, and ``printLinkedList()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None           # Start with an empty LL\n",
    "        self.length = 0\n",
    "\n",
    "    def appendNode(self, data):    # Either the beginning or the end\n",
    "        new_node = Node(data)      # Composition\n",
    "        if self.head == None:      # if LL is empty\n",
    "            self.head = new_node   # Add the 1st node\n",
    "            self.length += 1\n",
    "        else:\n",
    "            current = self.head\n",
    "            while current.next:    # Traverse to the end\n",
    "                current = current.next\n",
    "            current.next = new_node\n",
    "            self.length += 1   \n",
    "\n",
    "    def printLinkedList(self):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            print(current)\n",
    "            current = current.next\n",
    "\n",
    "    def __len__(self):\n",
    "        return f\"Length of LL: {self.length}\"\n",
    "\n",
    "LL = LinkedList()\n",
    "LL.appendNode(1)\n",
    "LL.appendNode(2)\n",
    "LL.appendNode(3)\n",
    "LL.appendNode(4)\n",
    "LL.printLinkedList()\n",
    "print(LL.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one more method:``insertNode(data, position)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedList(Node):\n",
    "    def __init__(self):\n",
    "        self.head = None         # Start with an empty ll\n",
    "        self.length = 0\n",
    "\n",
    "    def appendNode(self, data):\n",
    "        new_node = Node(data)    # Composition\n",
    "        if not self.head:                # if LL is empty\n",
    "            self.head = new_node         # Add the 1st node\n",
    "            self.length += 1\n",
    "        else:\n",
    "            current = self.head\n",
    "            while current.next:          # Traverse to the end\n",
    "                current = current.next\n",
    "            current.next = new_node\n",
    "            self.length += 1\n",
    "\n",
    "    def insertNode(self, data, pos): \n",
    "        if pos < 0 or pos > self.length:\n",
    "            print(\"Error: position out of range\")\n",
    "            return\n",
    "\n",
    "        new_node = Node(data)\n",
    "        \n",
    "        if pos == 1:               # Insert before the current head\n",
    "            new_node.next = self.head\n",
    "            self.head = new_node\n",
    "            self.length += 1\n",
    "            return     \n",
    "\n",
    "        current = self.head\n",
    "        index = 1                 # Insert in the middle\n",
    "        while current:            # Traverse to the appropriate position\n",
    "            if index == pos - 1:  # insert the new_node between pos-1 and pos\n",
    "                new_node.next = current.next\n",
    "                current.next = new_node\n",
    "                self.length += 1\n",
    "\n",
    "            current = current.next\n",
    "            index += 1\n",
    "   \n",
    "    def printLinkedList(self):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            print(current)\n",
    "            current = current.next\n",
    "\n",
    "    def __len__(self):\n",
    "        return f\"Length of LL: {self.length}\"\n",
    "\n",
    "LL = LinkedList()\n",
    "LL.appendNode(1)\n",
    "LL.appendNode(2)\n",
    "LL.appendNode(3)\n",
    "LL.appendNode(4)\n",
    "LL.printLinkedList()\n",
    "print(LL.__len__())\n",
    "LL.insertNode('N', 2)\n",
    "LL.printLinkedList()\n",
    "print(LL.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **stack** is a collection, i.e. it is a data structure that contains multiple elements. A stack is sometimes called a “last in, first out” or LIFO data structure, because the last item added is the first to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Stack class that has an attribute items, which is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack():\n",
    "    def __init__(self):\n",
    "        self.items = []         # Start with an empty list, i.e., we use List to implemt Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method ``push()`` that adds an item to the stack. The method takes item as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack():\n",
    "    def __init__(self):\n",
    "        self.items = []          \n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)  # Uses list's built-in method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method ``pop()`` that removes an item from the stack and returns the popped item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack():\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    " \n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.items.pop()  # Uses list's built-in method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method ``isEmpty()`` that returns True if the stack is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.items.pop()\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return len(self.items) == 0  # Uses list's built-in method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate a usage of Stack: add items 10, 20, 30, and remove all items until the stack is empty. Then calculate the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "s = Stack()    # s.items = []\n",
    "s.push(10)\n",
    "s.push(20)\n",
    "s.push(30)\n",
    "\n",
    "lst =[]\n",
    "while not s.isEmpty(): \n",
    "    lst.append(s.pop())\n",
    "print(lst)\n",
    "add = lambda a, b : a + b\n",
    "reduce(add, lst)     # Applies a function cumulatively to a sequence of items and returns a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, a **queue** is a line of customers waiting for service of some kind. In most cases, the first customer in line is the next customer to be served. There are exceptions, though. At airports, customers whose flights are leaving soon are sometimes taken from the middle of the queue. The rule that determines who goes next is called the **queueing policy**. The simplest queueing policy is called **FIFO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a class ``Queue`` that has attirbutes head and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.head = None     # Start with an empty queue\n",
    "        self.length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a method ``isEmpty()`` to the class Queue that returns True if the queue is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.length = 0\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return self.length == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a method ``enQueue()`` that adds data to the back of the queue, and a method ``__len__()`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.length = 0\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return self.length == 0\n",
    "\n",
    "    def enQueue(self, data):\n",
    "        node = Node(data)       # Composition\n",
    "        if self.head is None:   # If the queue is empty\n",
    "            self.head = node    # Add the first element\n",
    "        else:\n",
    "            last = self.head    # Start from the front of the queue and\n",
    "            while last.next:    # traverse to the end \n",
    "                last = last.next\n",
    "            last.next = node    # Add the new node to the end\n",
    "        self.length += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "qu = Queue()         # qu.head = None. Creats an empty queue\n",
    "print(qu.isEmpty())\n",
    "qu.enQueue(10)\n",
    "qu.enQueue(20)\n",
    "qu.enQueue(30)\n",
    "qu.enQueue(40)\n",
    "print(qu.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An open-source library developed by Meta (Facebook). It is widely used for Deep Learning, AI research, scientific computing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   # The Python package we import to use PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor: fundamental data structure in PyTorch — all the inputs, weights, biases, outputs, and gradients are tensors. A tensor is a multi-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy or convert data from Python lists, NumPy arrays, or scalar values into a PyTorch tensor\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data, dtype = torch.float32)  # int64 otherwise\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor with random numbers drawn from a uniform distribution on the interval [0,1)\n",
    "x = torch.rand(3, 2)  # Define the shape of the output tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape: a tuple of integers that describes its dimensions; Each element of the tuple gives the size of that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1)     # Creates a tensor from a scalar, e.g., integer ot float (no dimensions)\n",
    "print(x.shape) \n",
    "print(x.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1])   # Creates a 1-D tensor form a list\n",
    "print(x.shape)       \n",
    "print(x.ndim) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1]]) # Creates a 2-D tensor from a nested list: 1 row × 1 column matrix\n",
    "print(x.shape)       \n",
    "print(x.ndim) \n",
    "print(x.size(0))    # size along first dimension\n",
    "print(x.size(1))    # size along second dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3],  # Creates a 2-D tensor from a nested list: 2 rows × 3 columns   \n",
    "                  [4, 5, 6]]) # 2 samples; 3 features per sample  \n",
    "\n",
    "print(x.shape)   \n",
    "print(x.ndim)   \n",
    "print(x.size(0)) # size along the first dimension\n",
    "print(x.size(1)) # size along the second dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd system: Automatic differentiation engine. It tracks tensor operations to automatically compute gradients on tensors that requires_grad = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)   # Default: requires_grad = False\n",
    "y = x ** 2 + 1\n",
    "y.backward()          # Compute dy/dx = 2x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = x @ w\n",
    "x = torch.tensor([[1.0]])   # Creates a 2-D tensor with shape (1, 1) and value 1.0\n",
    "                            # Take the default (requires_grad = False). Here x is just input data (not a learnable parameter)\n",
    "\n",
    "w = torch.tensor([[2.0]], requires_grad = True)  \n",
    "                            # Track all operations on this tensor so we can compute its gradient later (means w is a learnable parameter)\n",
    "\n",
    "y = x @ w     # Performs matrix multiplication. y = x @ w = 1.0 × 2.0 = 2.0\n",
    "              # PyTorch records this operation in the computational gragh\n",
    "\n",
    "y.backward()  # Performs backpropagation (computes gradient of y w.r.t. all tensors that have requires_grad = True\n",
    "              # ∂y/∂w = x = 1.0. The computed gradient is stored in w.grad, which points in the direction of increasing loss\n",
    "print(w.grad)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MmBackward0: An internal function calculating the gradients of the Matrix multiplication (Mm) operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple training steps to show the loss decreasing and w converging toward the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Data and model parameter\n",
    "x = torch.tensor([[1.0]])        # Creates a 2-D tensor: 1 row × 1 column  \n",
    "y_true = torch.tensor([[3.0]])   # Target value\n",
    "w = torch.tensor([[2.0]], requires_grad = True)  # Model parameter (weight)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = x @ w                   # Performs matrix multiplication  \n",
    "\n",
    "# Compute loss\n",
    "loss = (y_pred - y_true) ** 2    # Scalar loss\n",
    "\n",
    "epoch = 0\n",
    "while (loss > 0.0001):\n",
    "    # Backward pass\n",
    "    loss.backward()              # Compute ∂loss/∂w, which points in the direction of increasing loss\n",
    "\n",
    "    # Update weight (Gradient Descent)\n",
    "    learning_rate = 0.1\n",
    "    with torch.no_grad():              # Temporarily turns off autograd for manual update\n",
    "        w -= learning_rate * w.grad    # w = w - lr * grad\n",
    "                                       # The gradient pulls w closer to the value that makes x @ w = y_true\n",
    "\n",
    "    # Zero gradients to prevent them from accumulating across iterations. (PyTorch accumulates gradients by default.)\n",
    "    w.grad.zero_()\n",
    "\n",
    "    # Using the updated w for the next forward pass\n",
    "    y_pred = x @ w   \n",
    "\n",
    "    # Compute loss \n",
    "    loss = (y_pred - y_true) ** 2       \n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(f\"Loss: {loss.item():.5f}\")\n",
    "print(\"Predicted output:\", y_pred)\n",
    "print(\"Epochs:\", epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer**: algorithms that adjust the learnable parameters (weights and biases) of a neural network to minimize the loss function. A popular optimizer is Stochastic Gradient Descent (SGD).\n",
    "#### **Neural Networks**: ``torch.nn`` provides building blocks: layers, activiation functions, and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: A simple 1-layer neural network with bias and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Input data and model parameter\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])   # Creates a 2-D tensor with a shape of (1, 3)\n",
    "y_true = torch.tensor([[2.0, 4.0]])   # Y = X @ W.T + B\n",
    "\n",
    "# Define a linear learning model: a single layer\n",
    "model = nn.Linear(3, 2) # By default, PyTorch always tracks the weights and bias of nn.Linear: requires_grad = True\n",
    "                        # in_features = 3  ==> This specifies the size of the expected last dimension of the input tensor\n",
    "                        # out_features = 2 ==> This specifies the size of the output dimension\n",
    "\n",
    "# Define loss function, Mean Squared Error\n",
    "loss_fn = nn.MSELoss()  # Creates a callable object -- loss_fn is a FUNCTION to be used to compute the loss \n",
    "\n",
    "# Create a Stochastic Gradient Descent (SGD) optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)  \n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x)          # Forward pass. Y = X @ W.T + B\n",
    "    loss = loss_fn(y_pred, y_true)  # Computes the loss value (a scalar tensor) AND\n",
    "                               # automatically builds the computation graph connecting the model parameters\n",
    "    optimizer.zero_grad()      # Resets gradients before computing new ones   \n",
    "    loss.backward()            # Compute gradients via backpropagation: ∂L/∂w, ∂L/∂b\n",
    "                               # These values are stored in model.weight.grad and model.bias.grad\n",
    "    optimizer.step()           # Update model weights using gradients: w = w - lr * dw, b = b - lr * db\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "print(model.weight.grad)\n",
    "print(model.bias.grad)\n",
    "print(model(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addmm: Addition of a Matrix and a Matrix Multiplication. An internal function that computes Y = X@W.T+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happy Thanksgiving!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No class next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "PyTorch Autograd System Explanation\n",
    "\n",
    "The Autograd system automatically computes gradients (derivatives) for tensors \n",
    "that have requires_grad=True. This is essential for training neural networks \n",
    "using backpropagation.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Create a tensor with requires_grad=True\n",
    "# This tells PyTorch to track all operations performed on this tensor\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(\"Step 1 - Creating tensor with requires_grad=True:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "print(f\"x.grad (before computation) = {x.grad}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Compute a simple function using that tensor\n",
    "# PyTorch automatically builds a computational graph to track this operation\n",
    "# Function: f(x) = 2x^3 + 3x^2 - 5x + 1\n",
    "y = 2 * x**3 + 3 * x**2 - 5 * x + 1\n",
    "print(\"Step 2 - Computing function using the tensor:\")\n",
    "print(f\"y = 2x³ + 3x² - 5x + 1\")\n",
    "print(f\"y = {y.item():.2f}\")\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")  # y also requires grad because it depends on x\n",
    "print()\n",
    "\n",
    "# Step 3: Call .backward() to compute gradients\n",
    "# This performs backpropagation: computes dy/dx for all tensors with requires_grad=True\n",
    "y.backward()\n",
    "print(\"Step 3 - Calling .backward() to compute gradients:\")\n",
    "print(\"Backpropagation completed!\")\n",
    "print()\n",
    "\n",
    "# Step 4: Access the gradient information in .grad\n",
    "# The gradient dy/dx = 6x² + 6x - 5\n",
    "# At x = 3.0: dy/dx = 6(3)² + 6(3) - 5 = 54 + 18 - 5 = 67\n",
    "print(\"Step 4 - Gradient information stored in .grad:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"x.grad.item() = {x.grad.item():.2f}\")\n",
    "print()\n",
    "print(\"Verification:\")\n",
    "print(f\"Manual calculation: dy/dx = 6x² + 6x - 5\")\n",
    "print(f\"At x = 3.0: dy/dx = 6(3)² + 6(3) - 5 = {6*3**2 + 6*3 - 5}\")\n",
    "print(f\"PyTorch computed: dy/dx = {x.grad.item():.2f}\")x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Does it matter what operation we do for y? What is the significance?\n",
    "\n",
    "ANSWER: The operation DOES matter for what gradient you get, but autograd \n",
    "works with ANY differentiable operation automatically!\n",
    "\n",
    "Key Points:\n",
    "1. Autograd works with any differentiable operation (+, -, *, /, **, sin, cos, exp, log, etc.)\n",
    "2. The specific operation determines what the gradient will be\n",
    "3. PyTorch automatically computes the correct gradient using the chain rule\n",
    "4. This is what makes neural network training possible - you can build complex \n",
    "   functions and PyTorch handles all the gradient calculations automatically\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION: Different operations produce different gradients\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Same input value\n",
    "x_val = 3.0\n",
    "\n",
    "# Example 1: Linear function y = 5x\n",
    "x1 = torch.tensor(x_val, requires_grad=True)\n",
    "y1 = 5 * x1\n",
    "y1.backward()\n",
    "print(f\"Operation: y = 5x\")\n",
    "print(f\"Gradient: dy/dx = 5\")\n",
    "print(f\"PyTorch computed: {x1.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Quadratic function y = x²\n",
    "x2 = torch.tensor(x_val, requires_grad=True)\n",
    "y2 = x2 ** 2\n",
    "y2.backward()\n",
    "print(f\"Operation: y = x²\")\n",
    "print(f\"Gradient: dy/dx = 2x = 2({x_val}) = {2*x_val}\")\n",
    "print(f\"PyTorch computed: {x2.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Exponential function y = e^x\n",
    "x3 = torch.tensor(x_val, requires_grad=True)\n",
    "y3 = torch.exp(x3)\n",
    "y3.backward()\n",
    "print(f\"Operation: y = e^x\")\n",
    "print(f\"Gradient: dy/dx = e^x = e^{x_val} ≈ {torch.exp(torch.tensor(x_val)).item():.2f}\")\n",
    "print(f\"PyTorch computed: {x3.grad.item():.2f}\")\n",
    "print()\n",
    "\n",
    "# Example 4: Trigonometric function y = sin(x)\n",
    "x4 = torch.tensor(x_val, requires_grad=True)\n",
    "y4 = torch.sin(x4)\n",
    "y4.backward()\n",
    "print(f\"Operation: y = sin(x)\")\n",
    "print(f\"Gradient: dy/dx = cos(x) = cos({x_val}) ≈ {torch.cos(torch.tensor(x_val)).item():.2f}\")\n",
    "print(f\"PyTorch computed: {x4.grad.item():.2f}\")\n",
    "print()\n",
    "\n",
    "# Example 5: Complex function (like our original example)\n",
    "x5 = torch.tensor(x_val, requires_grad=True)\n",
    "y5 = 2 * x5**3 + 3 * x5**2 - 5 * x5 + 1\n",
    "y5.backward()\n",
    "print(f\"Operation: y = 2x³ + 3x² - 5x + 1\")\n",
    "print(f\"Gradient: dy/dx = 6x² + 6x - 5 = 6({x_val})² + 6({x_val}) - 5 = {6*x_val**2 + 6*x_val - 5}\")\n",
    "print(f\"PyTorch computed: {x5.grad.item()}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SIGNIFICANCE:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. AUTOMATIC DIFFERENTIATION: PyTorch automatically computes gradients for \n",
    "   ANY differentiable operation - you don't need to manually derive formulas!\n",
    "\n",
    "2. CHAIN RULE: For complex functions (like neural networks), PyTorch uses \n",
    "   the chain rule to compute gradients through multiple layers automatically.\n",
    "\n",
    "3. FLEXIBILITY: You can use any combination of operations (polynomials, \n",
    "   trigonometric, exponential, etc.) and PyTorch will handle it.\n",
    "\n",
    "4. NEURAL NETWORKS: This is why we can train deep neural networks - each \n",
    "   layer can have different operations (linear, ReLU, sigmoid, etc.), and \n",
    "   PyTorch automatically computes how to update all parameters via backpropagation.\n",
    "\n",
    "5. THE OPERATION MATTERS: Different operations produce different gradients, \n",
    "   which affects how parameters are updated during training. This is why \n",
    "   choosing the right activation functions and loss functions is important!\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
